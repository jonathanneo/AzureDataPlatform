{"cells":[{"cell_type":"markdown","source":["##Cleansing Functions for String Columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b501a62-2606-4034-bdea-bbd9f2b12a67"}}},{"cell_type":"code","source":["%run \"../Util/Util Functions\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9b2dbce-052e-46f7-a605-6d5ef243ca58"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import when, col\nfrom pyspark.sql.functions import initcap, lower, upper\nfrom pyspark.sql.types import StringType\n\nimport re\nfrom rapidfuzz import fuzz\n\nclass CleanseString:\n  \"\"\"\n  Methods for cleaning and validating dataframe columns of string type.\n  \n  Attributes\n  ----------\n  error_value : str\n    Value to return on error of user defined function\n  \n  Methods\n  -------\n  \n  \"\"\"\n  \n  error_value = \"$$_CleansingTransformationFailed_$$\"\n  \n  # Cleaning Methods\n  \n  def cln_title_case(self, df, df_errors, columns: list, error_action: str, error_replace_value=None):\n    \"\"\"\n    Converts all strings to Proper Case for each column listed in the dataframe.\n    \n    Parameters\n    ----------\n    df : DataFrame\n    df_errors: DataFrame\n      Spark DataFrame to write errors to\n    columns : list\n      List of string columns to convert to Proper Case\n    error_action : str {'continue_and_null_value', 'continue_and_drop_row', 'continue_and_replace_value', 'stop'}\n    error_replace_value : str\n    \n    Returns\n    -------\n    df_cleaned : DataFrame\n      Cleaned Spark DataFrame\n    df_errors : DataFrame\n      DataFrame containing records where cleaning failed\n    \"\"\"\n    function_name = \"cln_title_case\"\n    \n    def titleCase(string: str):\n      newString = string.title()\n      return newString\n    \n    #Example is initcap was a UDF, built in automatically return null\n    titleCaseWrapper = util.safeUdf(titleCase, StringType(), self.error_value)\n    \n    for c_name in columns:\n        df = df.withColumn(c_name+\"__transformed__\", titleCaseWrapper(col(c_name)))\n        \n    df_cleaned, df_errors = util.handleErrors(df, df_errors, columns, function_name, error_action, self.error_value, error_replace_value)\n    \n    return df_cleaned, df_errors\n\n    \n  def cln_lower_case(self, df, df_errors, columns: list, error_action: str, error_replace_value=None):\n    \"\"\"\n    Converts all strings to lower case for each column listed in the dataframe.\n    \n    Parameters\n    ----------\n    df : DataFrame\n    df_errors: DataFrame\n    columns : list\n      List of string columns to convert to lower case\n    error_action : str {'continue_and_null_value', 'continue_and_drop_row', 'continue_and_replace_value', 'stop'}\n    error_replace_value : str\n    \n    Returns\n    -------\n    df_cleaned : DataFrame\n      Cleaned Spark DataFrame\n    df_errors : DataFrame\n      DataFrame containing records where cleaning failed\n    \"\"\"\n    \n    function_name = \"cln_lower_case\"\n    \n    def lowerCase(string: str):\n      newString = string.lower()\n      return newString\n    \n    #Example is initcap was a UDF, built in automatically return null\n    lowerCaseWrapper = util.safeUdf(lowerCase, StringType(), self.error_value)  \n    \n    for c_name in columns:\n        df = df.withColumn(c_name, lowerCaseWrapper(col(c_name)))\n        \n    df_cleaned, df_errors = util.handleErrors(df, df_errors, columns, function_name, error_action, self.error_value, error_replace_value)\n    \n    return df_cleaned, df_errors\n    \n    \n  def cln_upper_case(self, df, df_errors, columns: list, error_action: str, error_replace_value=None):\n    \n    \"\"\"\n    Converts all strings to UPPER CASE for each column listed in the dataframe.\n    \n    Parameters\n    ----------\n    df : DataFrame\n    df_errors: DataFrame\n    columns : list\n      List of string columns to convert to UPPER CASE\n    error_action : str {'continue_and_null_value', 'continue_and_drop_row', 'continue_and_replace_value', 'stop'}\n    error_replace_value : str\n    \n    Returns\n    -------\n    df_cleaned : DataFrame\n      Cleaned Spark DataFrame\n    df_errors : DataFrame\n      DataFrame containing records where cleaning failed\n    \"\"\"\n    function_name = \"cln_upper_case\"\n    \n    def upperCase(string: str):\n      newString = string.upper()\n      return newString\n    \n    #Example is initcap was a UDF, built in automatically return null\n    upperCaseWrapper = util.safeUdf(upperCase, StringType(), self.error_value)  \n    \n    for c_name in columns:\n        df = df.withColumn(c_name+\"__transformed__\", upperCaseWrapper(col(c_name)))\n        \n    df_cleaned, df_errors = util.handleErrors(df, df_errors, columns, function_name, error_action, self.error_value, error_replace_value)\n    \n    return df_cleaned, df_errors\n  \n  def cln_regex_replace(self, df, df_errors, columns: list, regex_exp: str, replace_string: str, error_action: str, error_replace_value=None):\n    \"\"\"\n    Replaces values matching a regex expression with the specified value for each column listed in the dataframe.\n    \n    Parameters\n    ----------\n    df : DataFrame\n    df_errors: DataFrame\n    columns : list\n      List of string columns to convert to UPPER CASE\n    regex_exp : str\n      Regular expression used to identify values to replace\n    replace_string : str\n      The value to replace strings matched by the regex\n    error_action : str {'continue_and_null_value', 'continue_and_drop_row', 'continue_and_replace_value', 'stop'}\n    error_replace_value : str\n    \n    Returns\n    -------\n    df_cleaned : DataFrame\n      Cleaned Spark DataFrame\n    df_errors : DataFrame\n      DataFrame containing records where cleaning failed\n    \"\"\"    \n    function_name = \"cln_regex_replace\"\n    \n    def reReplace(string: str):\n      newString = re.sub(regex_exp, replace_string, string)\n      return newString\n    \n    reReplaceWrapper = util.safeUdf(reReplace, StringType(), self.error_value)\n    \n    for c_name in columns:\n      df = df.withColumn(c_name+\"__transformed__\", reReplaceWrapper(col(c_name)))\n          \n    df_cleaned, df_errors = util.handleErrors(df, df_errors, columns, function_name, error_action, self.error_value, error_replace_value)\n    \n    return df_cleaned, df_errors\n\n  \n    def cln_fuzzy_match_replace(self, df, df_errors, columns: list, common_strings: list, similarity_threshold: float, error_action: str, error_replace_value=None):\n      \"\"\"\n      Replaces values that are similar to the first string in a common list for each column listed in the dataframe.\n    \n      Parameters\n      ----------\n      df : DataFrame\n      df_errors: DataFrame\n      columns : list\n        List of string columns to convert to UPPER CASE\n      common_strings : list\n        List of common strings to fuzzy search for\n      similarity_threshold : float [0,100]\n        The threshold for similarity before replacing\n      error_action : str {'continue_and_null_value', 'continue_and_drop_row', 'continue_and_replace_value', 'stop'}\n      error_replace_value : str\n    \n      Returns\n      -------\n      df_cleaned : DataFrame\n        Cleaned Spark DataFrame\n      df_errors : DataFrame\n        DataFrame containing records where cleaning failed\n      \"\"\"    \n      function_name = \"cln_fuzzy_match_replace\"\n      \n      def fuzzyMatch(string: str):\n        for common_string in common_strings:\n          if fuzz.ratio(string, common_string, score_cutoff=similarity_threshold, processor=None):\n            newString = common_string\n          else:\n              new_string = string\n          return new_string\n    \n      fuzzyMatchWrapper = util.safeUdf(fuzzyMatch, StringType(), self.error_value)\n    \n      for c_name in columns:\n        df = df.withColumn(c_name, fuzzyMatchWrapper(col(c_name)))\n          \n      df_cleaned, df_errors = util.handleErrors(df, df_errors, columns, function_name, error_action, self.error_value, error_replace_value)\n    \n      return df_cleaned, df_errors\n  \n  # TODO: Validating Methods"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff38485d-894c-4230-b0c6-6d5df09dc804"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"String","dashboards":[],"language":"python","widgets":{},"notebookOrigID":489289456851648}},"nbformat":4,"nbformat_minor":0}
