{"cells":[{"cell_type":"code","source":["  from pyspark.sql.functions import array, when, lit, current_timestamp\n  \n  class util:\n    \n    \n    def handleErrors(df, df_errors, columns: list, function_name, error_action: str, error_value, replace_value=None):\n      \n      def writeErrorMessage(errorColumns):\n        errorMessage = f\"{function_name} failed on Column(s):\"+\", \".join([column_name for column_name in errorColumns if column_name is not None])\n        return errorMessage\n    \n      udf_writeErrorMessage = udf(writeErrorMessage, StringType())\n      \n      # Generate filter\n      filterStringErrors = ' or '.join([c_name+\"__transformed__\" +f\"='{error_value}'\" for c_name in columns])\n      filterStringCleaned = ' and '.join([c_name+\"__transformed__\" +f\"<>'{error_value}'\" for c_name in columns])   \n      \n      \n      # Get Error Dataframe\n      temp_df_errors = df.where(filterStringErrors)\n      temp_df_errors = temp_df_errors.withColumn(\"__DataProcessingError__\", array(*[when(col(c_name+\"__transformed__\")==error_value, lit(c_name)) for c_name in columns]))\\\n                                    .withColumn(\"__DataProcessingError__\", udf_writeErrorMessage(col(\"__DataProcessingError__\")))\n      temp_df_errors = temp_df_errors.withColumn(\"__Function__\", lit(function_name))\n      temp_df_errors = temp_df_errors.withColumn(\"__CleanseTimestamp__\", current_timestamp())\n      temp_df_errors = temp_df_errors.drop(*[c_name+\"__transformed__\" for c_name in columns])\n      temp_df_errors = temp_df_errors.select(df_errors.columns)\n      df_errors= df_errors.select(df_errors.columns)\n      df_errors = df_errors.union(temp_df_errors)\n      \n      cleanedCols = []\n      for sel_col in df.columns:\n        if \"__transformed__\" not in sel_col: \n          cleanedCols.append(sel_col)\n      \n      # Get Cleaned DataFrame\n      if error_action.lower() == \"stop\":\n        if temp_df_errors is not None:\n          raise Exception('Errors found and ErrorAction is Stop')\n        else:\n          df_cleaned = df.drop(*columns) # remove original columns \n          for c_name in columns:\n            df_cleaned = df_cleaned.withColumnRenamed(c_name+\"__transformed__\", c_name)  # rename columns to original        \n      \n      elif error_action.lower() == \"continue_and_drop_row\":\n        # Get Cleaned DataFrame\n        df_cleaned = df.where(filterStringCleaned)\n        df_cleaned = df_cleaned.drop(*columns) # remove original columns  \n        for c_name in columns:\n          df_cleaned = df_cleaned.withColumnRenamed(c_name+\"__transformed__\", c_name)    # rename columns to original  \n      \n      elif error_action.lower() == \"continue_and_null_value\":\n        # Get Cleaned DataFrame\n        df_cleaned = df.replace(error_value, value=None, subset=[c_name+\"__transformed__\" for c_name in columns])     \n        df_cleaned = df_cleaned.drop(*columns) # remove original columns\n        for c_name in columns:\n          df_cleaned = df_cleaned.withColumnRenamed(c_name+\"__transformed__\", c_name)    # rename columns to original      \n        \n      elif error_action.lower() == \"continue_and_replace_value\":\n        df_cleaned = df.replace(error_value, value=replace_value, subset=[c_name+\"__transformed__\" for c_name in columns])\n        df_cleaned = df_cleaned.drop(*columns) # remove original columns\n        for c_name in columns:\n          df_cleaned = df_cleaned.withColumnRenamed(c_name+\"__transformed__\", c_name) # rename columns to original\n      \n      else:\n        raise Exception(f'Invalid Error Action: {error_action}')\n        \n      df_cleaned = df_cleaned.select(cleanedCols) # ensure correct order \n      return df_cleaned, df_errors\n  \n    def safeUdf(fn, dtype, error_value):\n      def _(*args):\n          try:\n            return fn(*args)\n          except:\n            # In future, return exception string\n            return error_value\n      return udf(_, dtype)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46f5e8d5-5b9f-41f3-842e-54a570ba9c3f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d115c5f-4a1d-4f28-814e-764463e8b4d5"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Util Functions","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3902255459150120}},"nbformat":4,"nbformat_minor":0}
